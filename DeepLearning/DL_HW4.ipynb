{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ouGSsKSpScP",
        "colab_type": "code",
        "outputId": "85abc619-c98b-4552-e43d-c3bf7f3b5be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hamu5NuqUG1",
        "colab_type": "code",
        "outputId": "8beb9699-f186-4d34-9484-0327c5ccddf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "path = keras.utils.get_file(\n",
        "    'pg5200.txt',\n",
        "    origin='http://www.gutenberg.org/cache/epub/5200/pg5200.txt')\n",
        "text = open(path).read().lower()\n",
        "print('Corpus length:', len(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 139056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi7EqH5EK2h9",
        "colab_type": "text"
      },
      "source": [
        "# Char gen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAwIyLsGx9ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# text = open('eneida.txt').read().lower()\n",
        "text = re.sub(\"[^\\S\\r\\n]+\", \" \", text)\n",
        "text = re.sub(\"â€“|;|\\\"\", \" \", text)\n",
        "text = re.sub(\"[\\n]+\", \"\\n\", text)\n",
        "text = re.sub(\"\\[\\d+\\]\", \"\", text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMzX4YPurrhi",
        "colab_type": "code",
        "outputId": "d78f70b5-0d06-4511-f01e-ae0a82786468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "maxlen = 30\n",
        "step = 3\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('Number of sequences:', len(sentences))\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 46015\n",
            "Unique characters: 56\n",
            "Vectorization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WynIvJIfsqM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D31Zfscus2K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5I25p1qs38j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnI0eLWss8Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4NCh3FgtClk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 60):\n",
        "  print('\\nepoch', epoch)\n",
        "  model.fit(x, y, batch_size=128, epochs=1)\n",
        "  if epoch < 5:\n",
        "    continue\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  generated_text = text[start_index: start_index + maxlen]\n",
        "  print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "  for temperature in [0.2, 0.5, 1.0]:\n",
        "    print('------ temperature:', temperature)\n",
        "    sys.stdout.write(generated_text)\n",
        "    for i in range(400):\n",
        "      sampled = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(generated_text):\n",
        "        sampled[0, t, char_indices[char]] = 1.\n",
        "      preds = model.predict(sampled, verbose=0)[0]\n",
        "      next_index = sample(preds, temperature)\n",
        "      next_char = chars[next_index]\n",
        "      generated_text += next_char\n",
        "      generated_text = generated_text[1:]\n",
        "      sys.stdout.write(next_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv41h7NPtGTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('my_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJi3RKQpHmz8",
        "colab_type": "code",
        "outputId": "a779c930-3f6c-4501-deaa-8882f5a5b181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"46015/46015 [==============================] - 17s 376us/step - loss: 0.7537\n",
        "--- Generating with seed: \"s friendly.  he's enjoyed his \"\n",
        "------ temperature: 0.2\n",
        "s friendly.  he's enjoyed his room, then doorward be never tooke the tad off at himself with them and the chief clerk. \n",
        "he was a little when he had done than there was not any was also that his mother to the project \n",
        "gutenberg-tm work was something from the couch. one did not the window and whine here for the chest of the window, \n",
        "making his father and that was something for him any parable of the family he would be able to grego\n",
        "------ temperature: 0.5\n",
        "mily he would be able to gregor the pain with the couch. obo\n",
        "the couch.\n",
        "un the furniture and sention\n",
        "middle of the bed, was that without defin door in the chair and dirked and that was so much of any stammest\n",
        "thing would grete have to pother and was something first have to th, project\n",
        "gutenberg-tm electronic work and that was to make thing covered it any of the bedly was happen at the couch, \n",
        "mor.ity any plaming ofa all the tame\n",
        "------ temperature: 1.0\n",
        "ty any plaming of all the tame doors. he can betted, was that gregor coversly by\n",
        "the project gutenberg-tm work without the couch bitter,\n",
        "and was amsalfaw agake us all the effort to be parents yon\n",
        "could him - they was  mambe intany to get be pait project gutenberg-tm work before. \n",
        "we haven to get rem in her room without precuairned.\n",
        "he was that would go dooward ran apperend for at the care thand from but on the floor to the proj\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'46015/46015 [==============================] - 17s 376us/step - loss: 0.7537\\n--- Generating with seed: \"s friendly.  he\\'s enjoyed his \"\\n------ temperature: 0.2\\ns friendly.  he\\'s enjoyed his room, then doorward be never tooke the tad off at himself with them and the chief clerk. he was a little when he had done than there was not any was also that his mother to the project gutenberg-tm work was something from the couch. one did not the window and whine here for the chest of the window, making his father and that was something for him any parable of the family he would be able to grego------ temperature: 0.5\\nmily he would be able to gregor the pain with the couch. obo\\nthe couch.\\nun the furniture and sention\\nmiddle of the bed, was that without defin door in the chair and dirked and that was so much of any stammest\\nthing would grete have to pother and was something first have to th, project\\ngutenberg-tm electronic work and that was to make thing covered it any of the bedly was happen at the couch, mor.ity any plaming of all the tame------ temperature: 1.0\\nty any plaming of all the tame doors. he can betted, was that gregor coversly by\\nthe project gutenberg-tm work\\nwithout\\nthe couch bitter, and\\nwas amsalfaw agake us all the effort to be parents yon\\ncould him - they was  mambe intany to get be pait project gutenberg-tm work before. we haven to get rem in her room without precuairned.\\nhe was that would go dooward ran apperend for at the care thand\\nfrom\\nbut on\\nthe floor to the proj'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh9_xNfkK5eO",
        "colab_type": "text"
      },
      "source": [
        "# Word gen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX1kNFuiK5E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "# text = open('eneida.txt').read().lower()\n",
        "text = re.sub(\"\\d+\", \"\", text)\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text = re.sub(\"[\\n]+\", \"\\n\", text)\n",
        "text = re.sub(\"\\[*\\]\", \"\", text)\n",
        "text = re.sub(\"[^\\S\\r\\n]+\", \" \", text)\n",
        "rgx = re.compile(\"([\\w][\\w']*\\w)\")\n",
        "text = rgx.findall(text)\n",
        "text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ2-Q_u7RiXr",
        "colab_type": "code",
        "outputId": "d8c4b99b-bf16-44bc-8116-97fb1420a851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEPVvgjkTJae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}